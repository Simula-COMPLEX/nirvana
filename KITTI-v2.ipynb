{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "floral-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from plot_keras_history import plot_history\n",
    "from keras.preprocessing.image import  img_to_array\n",
    "\n",
    "#from keras.models import Model, Input\n",
    "#from keras.layers import Dropout,Dense\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "#from keras.layers import Dropout\n",
    "\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score,\\\n",
    "    precision_score,recall_score,f1_score\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import itertools\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.stats import entropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "INPUT_SIZE = (1242, 375, 3)\n",
    "IMAGE_SIZE = (INPUT_SIZE[1], INPUT_SIZE[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb57941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec076e4690194455b628748c2f0d6df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f61565f7a6842a5a84d695c69e3eb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f439e812f94b97aab0cb700014a6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ROOT_PATH = '/Users/ozgur/Documents/KITTI/'\n",
    "def load_data():\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    folders = glob(ROOT_PATH+'*')\n",
    "    label_index = 0.0\n",
    "    for folder in folders:\n",
    "        files = glob(folder + '/*.png',recursive=True)\n",
    "        for i in tqdm(range(50)):\n",
    "            fname = files[i]\n",
    "            image = cv2.imread(fname)\n",
    "            image = cv2.resize(image, IMAGE_SIZE)\n",
    "            image = img_to_array(image)\n",
    "            data.append(image)\n",
    "            labels.append(label_index)\n",
    "        label_index += 1.0\n",
    "\n",
    "    labels = LabelBinarizer().fit_transform(labels)\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "    \n",
    "X, y = load_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b43e7",
   "metadata": {},
   "source": [
    "# Uncertainty-Aware Prediction Validator in Deep Learning Models for Cyber-Physical System Data\n",
    "\n",
    "## KITTI Classification\n",
    "\n",
    "This repository presents the experiments of the paper:\n",
    "\n",
    "`Uncertainty-Aware Prediction Validator in Deep Learning Models for Cyber-Physical System Data`\n",
    "\n",
    "**Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "technological-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropout(input_tensor, p=0.5, mc=False):\n",
    "    if mc:\n",
    "        return Dropout(p)(input_tensor, training=True)\n",
    "    else:\n",
    "        return Dropout(p)(input_tensor)\n",
    "\n",
    "def shannon_entropy(vals):\n",
    "    return -1*np.sum([ val*np.log2(val+0.000000000001) for val in vals])\n",
    "\n",
    "def get_model(mc=False, act=\"relu\", dropout_size=0.3, num_of_class=2, input_size=2):\n",
    "    #image_input = Input(shape=(width_shape, height_shape, 3))\n",
    "    \n",
    "    baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
    "                      input_tensor=Input(shape=INPUT_SIZE))\n",
    "    \n",
    "    # the head of the model\n",
    "    headModel = baseModel.output\n",
    "    headModel = AveragePooling2D(pool_size=(3, 3))(headModel)\n",
    "    headModel = Flatten(name=\"flatten\")(headModel)\n",
    "    headModel = Dense(32, activation=\"relu\")(headModel)\n",
    "    headModel = get_dropout(headModel, p=dropout_size, mc=mc)\n",
    "    headModel = Dense(num_of_class, activation=\"softmax\")(headModel)\n",
    "    # the actual model\n",
    "    model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "    \n",
    "    for layer in baseModel.layers:\n",
    "    \tlayer.trainable = False\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='Adam',metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9afb9b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_without_dropouts(model):\n",
    "    dropout_layers = [22]\n",
    "    initial_rate = model.layers[dropout_layers[0]].rate\n",
    "    print('inital_rate', initial_rate)\n",
    "    \n",
    "    # change dropout ratio to 0\n",
    "    # i.e. disable dropouts for training\n",
    "    for layer_idx in dropout_layers:\n",
    "        model.layers[layer_idx].rate = 0.0\n",
    "        \n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"rmsprop\",\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    mcp_save = ModelCheckpoint('KITTI_model.hdf5', save_best_only=True, \n",
    "                               monitor='val_accuracy', mode='max')\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_accuracy', \n",
    "                       patience=30, \n",
    "                       min_delta=0.001,\n",
    "                       verbose=1,\n",
    "                       restore_best_weights=True,\n",
    "                       mode='max')\n",
    "    \n",
    "    history_mc = model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epoch_size,\n",
    "              callbacks=[mcp_save,es],\n",
    "              verbose=1,\n",
    "              validation_data=(X_test, y_test))\n",
    "\n",
    "    # change dropout ratio to original value\n",
    "    # i.e. enable dropouts for prediction\n",
    "    for layer_idx in dropout_layers:\n",
    "        model.layers[layer_idx].rate = initial_rate\n",
    "\n",
    "    return model, history_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023565a",
   "metadata": {},
   "source": [
    "## Bayesian Neural Network model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "basic-chassis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inital_rate 0.13\n",
      "Epoch 1/2000\n",
      "10/10 [==============================] - 189s 20s/step - loss: 2.0397 - accuracy: 0.3800 - val_loss: 0.9911 - val_accuracy: 0.5000\n",
      "Epoch 2/2000\n",
      "10/10 [==============================] - 198s 21s/step - loss: 0.9565 - accuracy: 0.4300 - val_loss: 0.9245 - val_accuracy: 0.4400\n",
      "Epoch 3/2000\n",
      " 4/10 [===========>..................] - ETA: 1:12 - loss: 0.9346 - accuracy: 0.4750"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8dfb81fe49ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                           act=\"relu\")\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpred_model_mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_models_without_dropouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_model_mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f05cd6f5f4e4>\u001b[0m in \u001b[0;36mtrain_models_without_dropouts\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     23\u001b[0m                        mode='max')\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     history_mc = model.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m     26\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_size = 2000\n",
    "batch_size = 10\n",
    "\n",
    "pred_model_mc = get_model(mc=True, \n",
    "                          num_of_class=3, \n",
    "                          dropout_size=0.13,\n",
    "                          input_size=X.shape[1],\n",
    "                          act=\"relu\")\n",
    "\n",
    "pred_model_mc, history_mc = train_models_without_dropouts(pred_model_mc)\n",
    "\n",
    "plot_history(history_mc.history, interpolate=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f4083",
   "metadata": {},
   "source": [
    "## Let's find the uncertainty values of the each test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672f498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Monte carlo predictions\n",
    "mc_predictions = []\n",
    "for i in tqdm.tqdm(range(200)):\n",
    "    y_p = pred_model_mc.predict(X_test, batch_size=1000)\n",
    "    mc_predictions.append(y_p)\n",
    "\n",
    "max_means = []\n",
    "preds = []\n",
    "entropy_vals = []\n",
    "std_vals = []\n",
    "for idx in range(X_test.shape[0]):\n",
    "    px = np.array([p[idx] for p in mc_predictions])\n",
    "    #print(px.max(axis=1))\n",
    "    preds.append(px.mean(axis=0).argmax())\n",
    "    max_means.append(px.mean(axis=0).max())\n",
    "    prob_dist = []\n",
    "    for i, (prob, var) in enumerate(zip(px.mean(axis=0), px.std(axis=0))):\n",
    "        prob_dist.append(prob)\n",
    "    entropy_vals.append(shannon_entropy(prob_dist))\n",
    "    #entropy_vals.append(entropy(prob_dist, base=2))\n",
    "    std_vals.append(np.std(px.max(axis=1)))\n",
    "\n",
    "unc_ent_idx = np.flip((np.array(entropy_vals)).argsort()[-3:])\n",
    "unc_std_idx = np.flip((np.array(std_vals)).argsort()[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef3f0a5",
   "metadata": {},
   "source": [
    "## Research Questions\n",
    "We aim to explore uncertainty quantification using the Bayesian neural network with the entropy and Softmax prediction probability variance methods. So we form the following Research Questions (RQs) and design the experiments to answer them: \n",
    "- **RQ1** How can the model's decision making be characterized with uncertainty quantification?\n",
    "- **RQ2** Is there any correlation between uncertainty and classification performance?\n",
    "- **RQ3** How can the model's false labelling be predicted by another model using the uncertainty values?\n",
    "- **RQ4** What is the best strategy for the best dropout ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_iter in range(3):\n",
    "    idx = unc_ent_idx[idx_iter]\n",
    "    p0 = np.array([p[idx] for p in mc_predictions])\n",
    "    prob_txt = []\n",
    "    prob_txt.append(\"True: {}\".format(y_test[idx].argmax()))\n",
    "    prob_txt.append(\", Pred: {}\".format(p0.mean(axis=0).argmax()))\n",
    "\n",
    "    prob_dist = []\n",
    "\n",
    "    for i, (prob, var) in enumerate(zip(p0.mean(axis=0), p0.std(axis=0))):\n",
    "        prob_txt.append(\", Class {} prob: {:2.2f}\\%\".format(i, prob*100))\n",
    "        prob_dist.append(prob)\n",
    "    prob_txt.append(\", Ent: {:.2f}\".format(shannon_entropy(prob_dist)))\n",
    "\n",
    "    prob_txt = \" \".join(prob_txt).strip()\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12,3))\n",
    "\n",
    "    for i, ax in enumerate(fig.get_axes()):\n",
    "        sns.distplot( p0[:,i],ax=ax, bins=30, fit=norm, kde=False)\n",
    "        ax.set_xlabel(f\"class {i}\")\n",
    "\n",
    "    fig.suptitle(prob_txt)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a41d65",
   "metadata": {},
   "source": [
    "## RQ2: Is there any correlation between uncertainty and classification performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2128873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(data = X_test,\n",
    "                  columns=['f' + str(i) for i in range(X_test.shape[1])])\n",
    "df_results['y'] = y_test.argmax(axis=1)\n",
    "df_results['y_hat'] = preds\n",
    "df_results['unc_entropy'] = entropy_vals\n",
    "df_results['unc_std'] = std_vals\n",
    "\n",
    "discarded_ratio_list = np.linspace(0,0.6,num=50)\n",
    "\n",
    "df_results.sort_values(by=['unc_entropy'],ascending=True,inplace=True)\n",
    "\n",
    "acc_list = []\n",
    "for discarded_ratio in discarded_ratio_list:\n",
    "    tmp_df = df_results.head(np.int(df_results.shape[0] * (1-discarded_ratio)))\n",
    "    y = tmp_df.y.values\n",
    "    y_hat = tmp_df.y_hat.values\n",
    "    acc_list.append(accuracy_score(y,y_hat))\n",
    "plt.plot(discarded_ratio_list,acc_list, '-', marker = '.')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(df_results.y,df_results.y_hat)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b8d12",
   "metadata": {},
   "source": [
    "## RQ3: How can the model's false labelling be predicted by another model using the uncertainty values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE(sampling_strategy=0.5, random_state=10)\n",
    "\n",
    "X_unc = np.stack((entropy_vals,std_vals),axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "X_unc = scaler.fit_transform(X_unc)\n",
    "\n",
    "mc_ensemble_pred = np.array(mc_predictions).mean(axis=0).argmax(axis=1)\n",
    "wrong_labels = np.abs(y_test.argmax(axis=1) - mc_ensemble_pred)\n",
    "wrong_labels[np.where(wrong_labels > 0)] = 1\n",
    "\n",
    "X_w_train, X_w_test, y_w_train, y_w_test = train_test_split(X_unc, wrong_labels, \n",
    "                                                    test_size=0.3)\n",
    "\n",
    "X_w_train, y_w_train = oversample.fit_resample(X_w_train, y_w_train)\n",
    "\n",
    "clf = SVC(gamma=1000,kernel='rbf',C=1000,tol=1e-14, max_iter=1e8)\n",
    "clf.fit(X_w_train, y_w_train)\n",
    "\n",
    "ax = plot_decision_regions(X_w_train, y_w_train, clf=clf, legend=1,\n",
    "                      markers='+o',hide_spines=True,colors='red,gray')\n",
    "plt.xlim((-0.1,1.0))\n",
    "plt.ylim((-0.1,1.0))\n",
    "plt.xlabel('Entropy')\n",
    "plt.ylabel('Variance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad1074",
   "metadata": {},
   "source": [
    "## RQ4: What is the best strategy for the best dropout ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_class = 2\n",
    "fig, axes = plt.subplots(1, num_of_class, figsize=(20, 5), sharey=True)\n",
    "for i in range(num_of_class):\n",
    "    kde_plot_title = ' Positive'\n",
    "    if i == 1:\n",
    "        kde_plot_title = ' Negative'\n",
    "\n",
    "    subset = df_results.query('y==' + str(i) + ' and y_hat!=' + str(i))\n",
    "    sns.distplot(subset['unc_entropy'], hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 1, 'shade': True},ax=axes[i],\n",
    "                 hist_kws=dict(alpha=1),color=\"salmon\",\n",
    "                 label = r'$y \\ne h_{pred}(\\mathbf{x}) \\Rightarrow$ False' + kde_plot_title)\n",
    "    \n",
    "    subset = df_results.query('y==' + str(i) + ' and y_hat==' + str(i))\n",
    "    sns.distplot( subset['unc_entropy'], hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 1, 'shade': True},ax=axes[i],\n",
    "                 hist_kws=dict(alpha=1),color=\"limegreen\",\n",
    "                 label = '$y = h_{pred}(\\mathbf{x}) \\Rightarrow$ True' + kde_plot_title)\n",
    "    axes[i].legend(prop={'size': 16}, \n",
    "                           title = 'Class :' + str(class_names[i]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-loading",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
