{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floral-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from plot_keras_history import plot_history\n",
    "from keras.preprocessing.image import  img_to_array\n",
    "\n",
    "#from keras.models import Model, Input\n",
    "#from keras.layers import Dropout,Dense\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "#from keras.layers import Dropout\n",
    "\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score,\\\n",
    "    precision_score,recall_score,f1_score\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import itertools\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.stats import entropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "INPUT_SIZE = (1242, 375, 3)\n",
    "IMAGE_SIZE = (INPUT_SIZE[1], INPUT_SIZE[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c283a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec076e4690194455b628748c2f0d6df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f61565f7a6842a5a84d695c69e3eb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f439e812f94b97aab0cb700014a6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ROOT_PATH = '/Users/ozgur/Documents/KITTI/'\n",
    "def load_data():\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    folders = glob(ROOT_PATH+'*')\n",
    "    label_index = 0.0\n",
    "    for folder in folders:\n",
    "        files = glob(folder + '/*.png',recursive=True)\n",
    "        for i in tqdm(range(50)):\n",
    "            fname = files[i]\n",
    "            image = cv2.imread(fname)\n",
    "            image = cv2.resize(image, IMAGE_SIZE)\n",
    "            image = img_to_array(image)\n",
    "            data.append(image)\n",
    "            labels.append(label_index)\n",
    "        label_index += 1.0\n",
    "\n",
    "    labels = LabelBinarizer().fit_transform(labels)\n",
    "    data = np.array(data) / 255.0\n",
    "\n",
    "    return data, labels\n",
    "    \n",
    "X, y = load_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b43e7",
   "metadata": {},
   "source": [
    "# Uncertainty-Aware Prediction Validator in Deep Learning Models for Cyber-Physical System Data\n",
    "\n",
    "## KITTI Classification\n",
    "\n",
    "This repository presents the experiments of the paper:\n",
    "\n",
    "`Uncertainty-Aware Prediction Validator in Deep Learning Models for Cyber-Physical System Data`\n",
    "\n",
    "**Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "technological-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropout(input_tensor, p=0.5, mc=False):\n",
    "    if mc:\n",
    "        return Dropout(p)(input_tensor, training=True)\n",
    "    else:\n",
    "        return Dropout(p)(input_tensor)\n",
    "\n",
    "def shannon_entropy(vals):\n",
    "    return -1*np.sum([ val*np.log2(val+0.000000000001) for val in vals])\n",
    "\n",
    "def get_model(mc=False, act=\"relu\", dropout_size=0.3, num_of_class=2, input_size=2):\n",
    "    #image_input = Input(shape=(width_shape, height_shape, 3))\n",
    "    \n",
    "    baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
    "                      input_tensor=Input(shape=INPUT_SIZE))\n",
    "    \n",
    "    # the head of the model\n",
    "    headModel = baseModel.output\n",
    "    headModel = AveragePooling2D(pool_size=(3, 3))(headModel)\n",
    "    headModel = Flatten(name=\"flatten\")(headModel)\n",
    "    headModel = Dense(32, activation=\"relu\")(headModel)\n",
    "    headModel = get_dropout(headModel, p=dropout_size, mc=mc)\n",
    "    headModel = Dense(num_of_class, activation=\"softmax\")(headModel)\n",
    "    # the actual model\n",
    "    model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "    \n",
    "    for layer in baseModel.layers:\n",
    "    \tlayer.trainable = False\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='Adam',metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9afb9b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_without_dropouts(model):\n",
    "    dropout_layers = [22]\n",
    "    initial_rate = model.layers[dropout_layers[0]].rate\n",
    "    print('inital_rate', initial_rate)\n",
    "    \n",
    "    # change dropout ratio to 0\n",
    "    # i.e. disable dropouts for training\n",
    "    for layer_idx in dropout_layers:\n",
    "        model.layers[layer_idx].rate = 0.0\n",
    "        \n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=\"rmsprop\",\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    mcp_save = ModelCheckpoint('video_model.hdf5', save_best_only=True, \n",
    "                               monitor='val_accuracy', mode='max')\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_accuracy', \n",
    "                       patience=30, \n",
    "                       min_delta=0.001,\n",
    "                       verbose=1,\n",
    "                       restore_best_weights=True,\n",
    "                       mode='max')\n",
    "    \n",
    "    history_mc = model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epoch_size,\n",
    "              callbacks=[mcp_save,es],\n",
    "              verbose=1,\n",
    "              validation_data=(X_test, y_test))\n",
    "\n",
    "    # change dropout ratio to original value\n",
    "    # i.e. enable dropouts for prediction\n",
    "    for layer_idx in dropout_layers:\n",
    "        model.layers[layer_idx].rate = initial_rate\n",
    "\n",
    "    return model, history_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023565a",
   "metadata": {},
   "source": [
    "## Bayesian Neural Network model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-chassis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inital_rate 0.13\n",
      "Epoch 1/2000\n",
      "10/10 [==============================] - 189s 20s/step - loss: 2.0397 - accuracy: 0.3800 - val_loss: 0.9911 - val_accuracy: 0.5000\n",
      "Epoch 2/2000\n",
      " 4/10 [===========>..................] - ETA: 1:15 - loss: 0.9559 - accuracy: 0.4250"
     ]
    }
   ],
   "source": [
    "epoch_size = 2000\n",
    "batch_size = 10\n",
    "\n",
    "pred_model_mc = get_model(mc=True, \n",
    "                          num_of_class=3, \n",
    "                          dropout_size=0.13,\n",
    "                          input_size=X.shape[1],\n",
    "                          act=\"relu\")\n",
    "\n",
    "pred_model_mc, history_mc = train_models_without_dropouts(pred_model_mc)\n",
    "\n",
    "plot_history(history_mc.history, interpolate=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f4083",
   "metadata": {},
   "source": [
    "## Let's find the uncertainty values of the each test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte carlo predictions\n",
    "mc_predictions = []\n",
    "for i in tqdm.tqdm(range(200)):\n",
    "    y_p = pred_model_mc.predict(X_test, batch_size=1000)\n",
    "    mc_predictions.append(y_p)\n",
    "\n",
    "max_means = []\n",
    "preds = []\n",
    "entropy_vals = []\n",
    "std_vals = []\n",
    "for idx in range(X_test.shape[0]):\n",
    "    px = np.array([p[idx] for p in mc_predictions])\n",
    "    #print(px.max(axis=1))\n",
    "    preds.append(px.mean(axis=0).argmax())\n",
    "    max_means.append(px.mean(axis=0).max())\n",
    "    prob_dist = []\n",
    "    for i, (prob, var) in enumerate(zip(px.mean(axis=0), px.std(axis=0))):\n",
    "        prob_dist.append(prob)\n",
    "    entropy_vals.append(shannon_entropy(prob_dist))\n",
    "    #entropy_vals.append(entropy(prob_dist, base=2))\n",
    "    std_vals.append(np.std(px.max(axis=1)))\n",
    "\n",
    "unc_ent_idx = np.flip((np.array(entropy_vals)).argsort()[-3:])\n",
    "unc_std_idx = np.flip((np.array(std_vals)).argsort()[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef3f0a5",
   "metadata": {},
   "source": [
    "## Research Questions\n",
    "We aim to explore uncertainty quantification using the Bayesian neural network with the entropy and Softmax prediction probability variance methods. So we form the following Research Questions (RQs) and design the experiments to answer them: \n",
    "- **RQ1** How can the model's decision making be characterized with uncertainty quantification?\n",
    "- **RQ2** Is there any correlation between uncertainty and classification performance?\n",
    "- **RQ3** How can the model's false labelling be predicted by another model using the uncertainty values?\n",
    "- **RQ4** What is the best strategy for the best dropout ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_iter in range(3):\n",
    "    idx = unc_ent_idx[idx_iter]\n",
    "    p0 = np.array([p[idx] for p in mc_predictions])\n",
    "    prob_txt = []\n",
    "    prob_txt.append(\"True: {}\".format(y_test[idx].argmax()))\n",
    "    prob_txt.append(\", Pred: {}\".format(p0.mean(axis=0).argmax()))\n",
    "\n",
    "    prob_dist = []\n",
    "\n",
    "    for i, (prob, var) in enumerate(zip(p0.mean(axis=0), p0.std(axis=0))):\n",
    "        prob_txt.append(\", Class {} prob: {:2.2f}\\%\".format(i, prob*100))\n",
    "        prob_dist.append(prob)\n",
    "    prob_txt.append(\", Ent: {:.2f}\".format(shannon_entropy(prob_dist)))\n",
    "\n",
    "    prob_txt = \" \".join(prob_txt).strip()\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12,3))\n",
    "\n",
    "    for i, ax in enumerate(fig.get_axes()):\n",
    "        sns.distplot( p0[:,i],ax=ax, bins=30, fit=norm, kde=False)\n",
    "        ax.set_xlabel(f\"class {i}\")\n",
    "\n",
    "    fig.suptitle(prob_txt)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a41d65",
   "metadata": {},
   "source": [
    "## RQ2: Is there any correlation between uncertainty and classification performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2128873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(data = X_test,\n",
    "                  columns=['f' + str(i) for i in range(X_test.shape[1])])\n",
    "df_results['y'] = y_test.argmax(axis=1)\n",
    "df_results['y_hat'] = preds\n",
    "df_results['unc_entropy'] = entropy_vals\n",
    "df_results['unc_std'] = std_vals\n",
    "\n",
    "discarded_ratio_list = np.linspace(0,0.6,num=50)\n",
    "\n",
    "df_results.sort_values(by=['unc_entropy'],ascending=True,inplace=True)\n",
    "\n",
    "acc_list = []\n",
    "for discarded_ratio in discarded_ratio_list:\n",
    "    tmp_df = df_results.head(np.int(df_results.shape[0] * (1-discarded_ratio)))\n",
    "    y = tmp_df.y.values\n",
    "    y_hat = tmp_df.y_hat.values\n",
    "    acc_list.append(accuracy_score(y,y_hat))\n",
    "plt.plot(discarded_ratio_list,acc_list, '-', marker = '.')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(df_results.y,df_results.y_hat)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b8d12",
   "metadata": {},
   "source": [
    "## RQ3: How can the model's false labelling be predicted by another model using the uncertainty values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE(sampling_strategy=0.5, random_state=10)\n",
    "\n",
    "X_unc = np.stack((entropy_vals,std_vals),axis=1)\n",
    "scaler = MinMaxScaler()\n",
    "X_unc = scaler.fit_transform(X_unc)\n",
    "\n",
    "mc_ensemble_pred = np.array(mc_predictions).mean(axis=0).argmax(axis=1)\n",
    "wrong_labels = np.abs(y_test.argmax(axis=1) - mc_ensemble_pred)\n",
    "wrong_labels[np.where(wrong_labels > 0)] = 1\n",
    "\n",
    "X_w_train, X_w_test, y_w_train, y_w_test = train_test_split(X_unc, wrong_labels, \n",
    "                                                    test_size=0.3)\n",
    "\n",
    "X_w_train, y_w_train = oversample.fit_resample(X_w_train, y_w_train)\n",
    "\n",
    "clf = SVC(gamma=1000,kernel='rbf',C=1000,tol=1e-14, max_iter=1e8)\n",
    "clf.fit(X_w_train, y_w_train)\n",
    "\n",
    "ax = plot_decision_regions(X_w_train, y_w_train, clf=clf, legend=1,\n",
    "                      markers='+o',hide_spines=True,colors='red,gray')\n",
    "plt.xlim((-0.1,1.0))\n",
    "plt.ylim((-0.1,1.0))\n",
    "plt.xlabel('Entropy')\n",
    "plt.ylabel('Variance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad1074",
   "metadata": {},
   "source": [
    "## RQ4: What is the best strategy for the best dropout ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_class = 2\n",
    "fig, axes = plt.subplots(1, num_of_class, figsize=(20, 5), sharey=True)\n",
    "for i in range(num_of_class):\n",
    "    kde_plot_title = ' Positive'\n",
    "    if i == 1:\n",
    "        kde_plot_title = ' Negative'\n",
    "\n",
    "    subset = df_results.query('y==' + str(i) + ' and y_hat!=' + str(i))\n",
    "    sns.distplot(subset['unc_entropy'], hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 1, 'shade': True},ax=axes[i],\n",
    "                 hist_kws=dict(alpha=1),color=\"salmon\",\n",
    "                 label = r'$y \\ne h_{pred}(\\mathbf{x}) \\Rightarrow$ False' + kde_plot_title)\n",
    "    \n",
    "    subset = df_results.query('y==' + str(i) + ' and y_hat==' + str(i))\n",
    "    sns.distplot( subset['unc_entropy'], hist = False, kde = True,\n",
    "                 kde_kws = {'linewidth': 1, 'shade': True},ax=axes[i],\n",
    "                 hist_kws=dict(alpha=1),color=\"limegreen\",\n",
    "                 label = '$y = h_{pred}(\\mathbf{x}) \\Rightarrow$ True' + kde_plot_title)\n",
    "    axes[i].legend(prop={'size': 16}, \n",
    "                           title = 'Class :' + str(class_names[i]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-loading",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
